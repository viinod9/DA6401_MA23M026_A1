{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iwR4Lf0NBJb",
        "outputId": "eb543b11-cb3f-45bb-e66e-e10e0d78b498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.7)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.22.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()\n",
        "wandb.login(key=\"acdc26d2fc17a56e83ea3ae6c10e496128dee648\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCsOszAQNIAM",
        "outputId": "a7c13d50-272f-4d0a-9acd-c41eb409f537"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mviinod9\u001b[0m (\u001b[33mviinod9-iitm\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Initialize WandB\n",
        "wandb.login()  # Ensure you are logged in\n",
        "wandb.init(project='Fashion-MNIST-Visualization', entity='viinod9-iitm')\n",
        "\n",
        "# Load the Fashion-MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Define class labels\n",
        "class_labels = {\n",
        "    0: \"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\",\n",
        "    5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle boot\"\n",
        "}\n",
        "\n",
        "# Select one sample per class\n",
        "sample_images = []\n",
        "sample_labels = []\n",
        "\n",
        "for label in range(10):\n",
        "    idx = next(i for i, j in enumerate(y_train) if j == label)  # Find first occurrence\n",
        "    sample_images.append(X_train[idx])\n",
        "    sample_labels.append(class_labels[label])\n",
        "\n",
        "# Plot and log images to WandB\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
        "fig.suptitle(\"Fashion-MNIST Samples\", fontsize=16)\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(sample_images[i], cmap=\"gray\")\n",
        "    ax.set_title(sample_labels[i], fontsize=10)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Log images to WandB\n",
        "wandb.log({\"Fashion-MNIST Samples\": [wandb.Image(img, caption=label) for img, label in zip(sample_images, sample_labels)]})\n",
        "\n",
        "# Split data for training and validation\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Flatten images\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "# Normalize images\n",
        "X_train, X_valid, X_test = X_train / 255.0, X_valid / 255.0, X_test / 255.0\n",
        "\n",
        "# One-hot encoding function\n",
        "def one_hot_encod(arr):\n",
        "    mat = np.zeros((len(arr), 10))\n",
        "    for i in range(len(arr)):\n",
        "        mat[i, arr[i]] = 1\n",
        "    return mat\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "Y_train, Y_valid, Y_test = one_hot_encod(y_train), one_hot_encod(y_valid), one_hot_encod(y_test)\n",
        "\n",
        "# Print dataset shapes\n",
        "print(\"Training images shape:\", X_train.shape)\n",
        "print(\"Training labels shape:\", Y_train.shape)\n",
        "print(\"Validation images shape:\", X_valid.shape)\n",
        "print(\"Validation labels shape:\", Y_valid.shape)\n",
        "print(\"Testing images shape:\", X_test.shape)\n",
        "print(\"Testing labels shape:\", Y_test.shape)\n",
        "\n",
        "# Finish WandB run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981
        },
        "id": "DAfKz1A4NPSv",
        "outputId": "d0111722-4bb9-4d35-84d1-e72ee9a14783"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250303_183701-jbm0klgp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/viinod9-iitm/Fashion-MNIST-Visualization/runs/jbm0klgp' target=\"_blank\">royal-deluge-1</a></strong> to <a href='https://wandb.ai/viinod9-iitm/Fashion-MNIST-Visualization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/viinod9-iitm/Fashion-MNIST-Visualization' target=\"_blank\">https://wandb.ai/viinod9-iitm/Fashion-MNIST-Visualization</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/viinod9-iitm/Fashion-MNIST-Visualization/runs/jbm0klgp' target=\"_blank\">https://wandb.ai/viinod9-iitm/Fashion-MNIST-Visualization/runs/jbm0klgp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training images shape: (48000, 784)\n",
            "Training labels shape: (48000, 10)\n",
            "Validation images shape: (12000, 784)\n",
            "Validation labels shape: (12000, 10)\n",
            "Testing images shape: (10000, 784)\n",
            "Testing labels shape: (10000, 10)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing up..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-7c9eb5b7b9de>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m# Finish WandB run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mfinish\u001b[0;34m(exit_code, quiet)\u001b[0m\n\u001b[1;32m   4130\u001b[0m     \"\"\"\n\u001b[1;32m   4131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4132\u001b[0;31m         \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexit_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexit_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquiet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDummy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_attaching\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mfinish\u001b[0;34m(self, exit_code, quiet)\u001b[0m\n\u001b[1;32m   2104\u001b[0m                 ),\n\u001b[1;32m   2105\u001b[0m             )\n\u001b[0;32m-> 2106\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexit_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2108\u001b[0m     def _finish(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36m_finish\u001b[0;34m(self, exit_code)\u001b[0m\n\u001b[1;32m   2125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2126\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2127\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_atexit_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexit_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexit_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2129\u001b[0m             \u001b[0;31m# Run hooks that should happen after the last messages to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36m_atexit_cleanup\u001b[0;34m(self, exit_code)\u001b[0m\n\u001b[1;32m   2352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2353\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2354\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36m_on_finish\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2609\u001b[0m         ) as progress_printer:\n\u001b[1;32m   2610\u001b[0m             \u001b[0;31m# Wait for the run to complete.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2611\u001b[0;31m             wait_with_progress(\n\u001b[0m\u001b[1;32m   2612\u001b[0m                 \u001b[0mexit_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2613\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/mailbox/wait_with_progress.py\u001b[0m in \u001b[0;36mwait_with_progress\u001b[0;34m(handle, timeout, progress_after, display_progress)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mEquivalent\u001b[0m \u001b[0mto\u001b[0m \u001b[0mpassing\u001b[0m \u001b[0ma\u001b[0m \u001b[0msingle\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mwait_all_with_progress\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \"\"\"\n\u001b[0;32m---> 23\u001b[0;31m     return wait_all_with_progress(\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/mailbox/wait_with_progress.py\u001b[0m in \u001b[0;36mwait_all_with_progress\u001b[0;34m(handle_list, timeout, progress_after, display_progress)\u001b[0m\n\u001b[1;32m     84\u001b[0m             )\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0masyncio_compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress_loop_with_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/asyncio_compat.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def init_network( num_hidden_layer, num_nodes_hidden_layers, weight, input_size, output_size ):\n",
        "    network_size = []\n",
        "    for i in range(num_hidden_layer):\n",
        "      network_size.append(num_nodes_hidden_layers)\n",
        "    size = [input_size] + network_size + [output_size]\n",
        "    theta0 = {}\n",
        "    if weight == 'random':\n",
        "      for i in range(1, num_hidden_layer+2):\n",
        "        theta0['W' + str(i)] = np.random.randn(size[i], size[i-1])\n",
        "        theta0['b' + str(i)] = np.random.randn(size[i], 1)\n",
        "    if weight == 'xavier':\n",
        "      for i in range(1, num_hidden_layer+2):\n",
        "          theta0[\"W\" + str(i)] = np.random.randn(size[i], size[i-1])*(np.sqrt(2/(size[i-1])))\n",
        "          theta0[\"b\" + str(i)] = np.random.randn(size[i], 1)*(np.sqrt(2/(size[i-1])))\n",
        "\n",
        "    return theta0\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 /(1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "def tanh(x):\n",
        "\treturn np.tanh(x)\n",
        "\n",
        "def deriv_sigmoid(x):\n",
        "  return sigmoid(x)*(1-sigmoid(x))\n",
        "\n",
        "def deriv_relu(x):\n",
        "  return np.where(x <= 0, 0, 1)\n",
        "\n",
        "def deriv_tanh(x):\n",
        "  return (1 - (tanh(x))**2)\n",
        "\n",
        "def softmax(x):\n",
        "  x = x - np.max(x)\n",
        "  return np.exp(x)/np.sum(np.exp(x),axis=0)\n",
        "\n",
        "def activation(x, activ_fun):\n",
        "  if activ_fun == 'sigmoid':\n",
        "    return sigmoid(x)\n",
        "  elif activ_fun == 'relu':\n",
        "    return relu(x)\n",
        "  else:\n",
        "    return tanh(x)\n",
        "\n",
        "def grad_activation(x, activ_fun):\n",
        "  if activ_fun == 'sigmoid':\n",
        "    return deriv_sigmoid(x)\n",
        "  elif activ_fun == 'relu':\n",
        "    return deriv_relu(x)\n",
        "  else:\n",
        "    return deriv_tanh(x)\n",
        "\n",
        "def cross_entropy(y_actual, y_pred):\n",
        "    epsilon = 1e-15  # to prevent log(0) which is undefined\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "    loss = -np.mean(np.sum(y_actual * np.log(y_pred), axis=0))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def forword_prop(x,theta,activ_fun, num_hidden_layer,input_size):\n",
        "  a = {}\n",
        "  a['a'+str(0)] = np.zeros((input_size,1))\n",
        "  h = {'h0':x}\n",
        "  for i in range(1,num_hidden_layer+1):\n",
        "    a[\"a\"+str(i)] = np.dot(theta['W'+str(i)],h['h'+str(i-1)]) + theta['b'+str(i)]\n",
        "    h['h'+str(i)] = activation(a[\"a\"+str(i)], activ_fun)\n",
        "  a['a'+str(num_hidden_layer+1)] = np.dot(theta['W'+str(num_hidden_layer+1)],h['h'+str(num_hidden_layer)]) + theta['b'+str(num_hidden_layer+1)]\n",
        "  y_pred = softmax(a['a'+str(num_hidden_layer+1)])\n",
        "\n",
        "  return a,h,y_pred"
      ],
      "metadata": {
        "id": "I57OnpQQP0q-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def back_prop(x, y_actual, activ_fun, num_hidden_layer, theta,input_size):\n",
        "  a,h,y_pred = forword_prop(x,theta,activ_fun, num_hidden_layer,input_size)\n",
        "  grad_W_b = {}\n",
        "  grad_h_a = {}\n",
        "  grad_h_a['a'+str(num_hidden_layer+1)] = -1*(y_actual - y_pred)\n",
        "  for i in range(num_hidden_layer+1, 0, -1):\n",
        "    grad_W_b['W'+str(i)] = np.dot(grad_h_a['a'+str(i)],h['h'+str(i-1)].T)\n",
        "    grad_W_b['b'+str(i)] = grad_h_a['a'+str(i)]  #np.mean(grad_h_a['a'+str(i)]#,axis = 0, keepdims=True)\n",
        "    grad_h_a['h'+str(i-1)] = np.dot(theta['W'+str(i)].T,grad_h_a['a'+str(i)] )\n",
        "    grad_h_a['a'+str(i-1)] = grad_h_a['h'+str(i-1)]*(grad_activation(a['a'+str(i-1)], activ_fun))\n",
        "\n",
        "  return grad_W_b\n",
        "\n",
        "\n",
        "def calculate_accuracy(X_test,y_test,theta_new,activ_fun, num_hidden_layer,input_size):\n",
        "  len = X_test.shape[0]\n",
        "  correct_predictions = 0\n",
        "  for i in range(len):\n",
        "    X_tes = X_test[i,:].reshape(-1, 1)\n",
        "    a,h,y_test_pred = forword_prop(X_tes,theta_new, activ_fun, num_hidden_layer,input_size)\n",
        "\n",
        "    y1 = np.argmax(y_test[i,:].reshape(-1, 1), axis = 0)\n",
        "    y2 = np.argmax(y_test_pred, axis = 0)\n",
        "\n",
        "    if y1 == y2:\n",
        "      correct_predictions += 1\n",
        "  accuracy = correct_predictions /y_test.shape[0]\n",
        "\n",
        "  return accuracy\n",
        "\n",
        "\"\"\"Optimizers\"\"\"\n",
        "\n",
        "# Stochastic Gradient Decent\n",
        "def sgd(lr, x_train, y_train, X_valid, Y_valid, epochs, activ_fun, num_hidden_layer,num_nodes_hidden_layers, weight,batch_size, input_size, output_size):\n",
        "  theta = init_network( num_hidden_layer, num_nodes_hidden_layers, weight, input_size, output_size )\n",
        "  losses = []  # List to store the loss for each epoch\n",
        "  length = x_train.shape[0]\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      arr = np.arange(length)\n",
        "      np.random.shuffle(arr)\n",
        "      no_of_pts = 0\n",
        "      grad_new = {key: np.zeros_like(value) for key, value in theta.items()}\n",
        "      for j in range(length):\n",
        "          no_of_pts += 1\n",
        "          x = x_train[arr[j],:].reshape(-1, 1)\n",
        "          y_actual = y_train[arr[j],:].reshape(-1, 1)\n",
        "          grad = back_prop(x, y_actual, activ_fun, num_hidden_layer, theta,input_size)\n",
        "          for k in range(1, num_hidden_layer + 2):\n",
        "            grad_new['W' + str(k)] += grad['W' + str(k)]\n",
        "            grad_new['b' + str(k)] += grad['b' + str(k)]\n",
        "          if no_of_pts % batch_size == 0:\n",
        "            for i in range(1, num_hidden_layer + 2):\n",
        "                theta['W' + str(i)] -= lr * grad_new['W' + str(i)]\n",
        "                theta['b' + str(i)] -= lr * grad_new['b' + str(i)]\n",
        "\n",
        "      # Compute and store the loss and accuracy for this epoch\n",
        "      acc = calculate_accuracy(x_train,y_train,theta,activ_fun, num_hidden_layer,input_size)\n",
        "      a, _, y_pred = forword_prop(x_train.T,theta,activ_fun, num_hidden_layer,input_size)\n",
        "      loss = cross_entropy(y_train.T, y_pred)\n",
        "      losses.append(loss)\n",
        "      validation_accuracy = calculate_accuracy(X_valid,Y_valid,theta,activ_fun, num_hidden_layer,input_size)\n",
        "\n",
        "\n",
        "      print(f\"Epoch {epoch+1}, train Loss: {loss} , train accuracy: {acc*100}, validation accuracy: {validation_accuracy*100}\")\n",
        "\n",
        "\n",
        "  # Plot the loss function over epochs\n",
        "  plt.plot(losses)\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('Training Loss over Epochs')\n",
        "  plt.show()\n",
        "\n",
        "  return theta\n",
        "\n",
        "# Momentum based gradient decent\n",
        "\n",
        "def momentum(lr, theta, x_train, y_train, X_valid, Y_valid, activ_fun, epochs, num_hidden_layer, input_size, batch_size, beta=0.9):\n",
        "    losses = []  # List to store the loss for each epoch\n",
        "    length = x_train.shape[0]\n",
        "    v = {key: np.zeros_like(value) for key, value in theta.items()}  # Initialize velocities\n",
        "    prev_v = {key: np.zeros_like(value) for key, value in theta.items()}\n",
        "    for epoch in range(epochs):  # Adjust the number of epochs as needed\n",
        "        no_of_pts = 0\n",
        "        loss = 0\n",
        "        itr = 0\n",
        "        no_of_batch = 0\n",
        "        grad_new = {key: np.zeros_like(value) for key, value in theta.items()}\n",
        "        for j in range(length):\n",
        "            no_of_pts += 1\n",
        "            x = x_train[j, :].reshape(-1, 1)\n",
        "            y_actual = y_train[j, :].reshape(-1, 1)\n",
        "            grad = back_prop(x, y_actual,activ_fun, num_hidden_layer, theta, input_size)\n",
        "            for i in range(num_hidden_layer+1, 0, -1):\n",
        "                grad_new['W'+str(i)] += grad['W'+str(i)]\n",
        "                grad_new['b'+str(i)] += grad['b'+str(i)]\n",
        "\n",
        "            if no_of_pts % batch_size == 0:\n",
        "              no_of_batch += 1\n",
        "              for i in range(1, num_hidden_layer + 2):\n",
        "                  # Update velocity\n",
        "                  v['W' + str(i)] = beta * prev_v['W' + str(i)] + lr * grad_new['W'+str(i)]\n",
        "                  v['b' + str(i)] = beta * prev_v['b' + str(i)] + lr * grad_new['b'+str(i)]\n",
        "\n",
        "              for i in range(1, num_hidden_layer + 2):\n",
        "                  # Update parameters\n",
        "                  theta['W' + str(i)] -= v['W' + str(i)]\n",
        "                  theta['b' + str(i)] -= v['b' + str(i)]\n",
        "\n",
        "              for i in range(1, num_hidden_layer + 2):\n",
        "                  prev_v['W' + str(i)] = v['W' + str(i)]\n",
        "                  prev_v['b' + str(i)] = v['b' + str(i)]\n",
        "\n",
        "        # Compute and store the loss for this epoch\n",
        "                  # x_batch = X_train[itr:itr+batch_size].T\n",
        "                  # y_batch = Y_train[itr:itr+batch_size].T\n",
        "                  # itr += batch_size\n",
        "                  # a, _, y_pred = forword_prop(x_batch, theta,activ_fun, num_hidden_layer, input_size)\n",
        "                  # loss += cross_entropy(y_batch, y_pred)#-np.mean(np.sum(y_batch.T * np.log(y_pred),axis = 0))\n",
        "        acc = calculate_accuracy(x_train,y_train,theta,activ_fun, num_hidden_layer,input_size)\n",
        "        a, _, y_pred = forword_prop(x_train.T,theta,activ_fun, num_hidden_layer,input_size)\n",
        "        loss = cross_entropy(y_train.T, y_pred)\n",
        "        losses.append(loss)\n",
        "        validation_accuracy = calculate_accuracy(X_valid,Y_valid,theta,activ_fun, num_hidden_layer,input_size)\n",
        "\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, train Loss: {loss} , train accuracy: {acc*100}, validation accuracy: {validation_accuracy*100}\")\n",
        "\n",
        "    # Plot the loss function over epochs\n",
        "    plt.plot(losses)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss over Epochs')\n",
        "    plt.show()\n",
        "\n",
        "    return theta\n",
        "\n",
        "# Nestrov acelarated gradient decent\n",
        "def nag(lr, theta, x_train, y_train, X_valid, Y_valid, activ_fun, epochs, num_hidden_layer, input_size, batch_size, beta=0.9):\n",
        "    losses = []\n",
        "    length = x_train.shape[0]\n",
        "    v = {key: np.zeros_like(value) for key, value in theta.items()}\n",
        "    prev_v = {key: np.zeros_like(value) for key, value in theta.items()}\n",
        "    for epoch in range(epochs):\n",
        "        grad_new = {key: np.zeros_like(value) for key, value in theta.items()}\n",
        "        no_of_pts = 0\n",
        "        no_of_batch = 0\n",
        "        loss = 0\n",
        "        itr = 0\n",
        "        for j in range(length):\n",
        "            no_of_pts += 1\n",
        "            x = x_train[j, :].reshape(-1, 1)\n",
        "            y_actual = y_train[j, :].reshape(-1, 1)\n",
        "\n",
        "            for i in range(1, num_hidden_layer + 2):\n",
        "              theta['W' + str(i)] -= beta * prev_v['W' + str(i)]\n",
        "              theta['b' + str(i)] -= beta * prev_v['b' + str(i)]\n",
        "\n",
        "            grad = back_prop(x, y_actual,activ_fun, num_hidden_layer, theta, input_size)\n",
        "            for i in range(num_hidden_layer+1, 0, -1):\n",
        "                grad_new['W'+str(i)] += grad['W'+str(i)]\n",
        "                grad_new['b'+str(i)] += grad['b'+str(i)]\n",
        "\n",
        "            if no_of_pts % batch_size == 0:\n",
        "              no_of_batch += 1\n",
        "              for i in range(1, num_hidden_layer + 2):\n",
        "                  # Update velocity\n",
        "                  v['W' + str(i)] = beta * prev_v['W' + str(i)] + lr * grad_new['W'+str(i)]\n",
        "                  v['b' + str(i)] = beta * prev_v['b' + str(i)] + lr * grad_new['b'+str(i)]\n",
        "\n",
        "              for i in range(1, num_hidden_layer + 2):\n",
        "                  # Update parameters\n",
        "                  theta['W' + str(i)] -= v['W' + str(i)]\n",
        "                  theta['b' + str(i)] -= v['b' + str(i)]\n",
        "\n",
        "              for i in range(1, num_hidden_layer + 2):\n",
        "                  prev_v['W' + str(i)] = v['W' + str(i)]\n",
        "                  prev_v['b' + str(i)] = v['b' + str(i)]\n",
        "\n",
        "                  # x_batch = X_train[itr:itr+batch_size].T\n",
        "                  # y_batch = Y_train[itr:itr+batch_size]\n",
        "                  # itr += batch_size\n",
        "                  # a, _, y_pred = forword_prop(x_batch, theta,activ_fun, num_hidden_layer, input_size)\n",
        "                  # epsilon = 1e-15  # to prevent log(0) which is undefined\n",
        "                  # y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "                  # loss += -np.mean(np.sum(y_batch.T * np.log(y_pred),axis = 0))\n",
        "        acc = calculate_accuracy(x_train,y_train,theta,activ_fun, num_hidden_layer,input_size)\n",
        "        a, _, y_pred = forword_prop(x_train.T,theta,activ_fun, num_hidden_layer,input_size)\n",
        "        loss = cross_entropy(y_train.T, y_pred)\n",
        "        losses.append(loss)\n",
        "        validation_accuracy = calculate_accuracy(X_valid,Y_valid,theta,activ_fun, num_hidden_layer,input_size)\n",
        "\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, train Loss: {loss} , train accuracy: {acc*100}, validation accuracy: {validation_accuracy*100}\")\n",
        "\n",
        "\n",
        "    # Plot the loss function over epochs\n",
        "    plt.plot(losses)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss over Epochs')\n",
        "    plt.show()\n",
        "\n",
        "    return theta\n",
        "\n",
        "# RMSprop optimizer\n",
        "\n",
        "def opt_RMSprop(lr, theta, x_train, y_train,X_valid, Y_valid,activ_fun, epochs, num_hidden_layer, input_size,batch_size, eps=0.01, beta=0.9):\n",
        "    losses = []\n",
        "    length = x_train.shape[0]\n",
        "    v = {key: np.zeros_like(value) for key, value in theta.items()}\n",
        "    prev_v = {key: np.zeros_like(value) for key, value in theta.items()}\n",
        "    for epoch in range(epochs):\n",
        "        no_of_pts = 0\n",
        "        loss = 0\n",
        "        grad_new = {key: np.zeros_like(value) for key, value in theta.items()}\n",
        "        for j in range(length):\n",
        "            no_of_pts += 1\n",
        "            x = x_train[j, :].reshape(-1, 1)\n",
        "            y_actual = y_train[j, :].reshape(-1, 1)\n",
        "            grad = back_prop(x, y_actual,activ_fun, num_hidden_layer, theta, input_size)\n",
        "            for i in range(num_hidden_layer+1, 0, -1):\n",
        "                grad_new['W'+str(i)] += grad['W'+str(i)]\n",
        "                grad_new['b'+str(i)] += grad['b'+str(i)]\n",
        "\n",
        "            if no_of_pts % batch_size == 0:\n",
        "\n",
        "              for i in range(1, num_hidden_layer + 2):\n",
        "                  # Update velocity\n",
        "                  v['W' + str(i)] = beta * prev_v['W' + str(i)] + (1-beta)* ((grad_new['W'+str(i)])**2)\n",
        "                  v['b' + str(i)] = beta * prev_v['b' + str(i)] + (1-beta)* ((grad_new['b'+str(i)])**2)\n",
        "\n",
        "              for i in range(1, num_hidden_layer + 2):\n",
        "                  # Update parameters\n",
        "                  theta['W' + str(i)] -= (lr/ (np.sqrt(v['W' + str(i)] + eps)) )*grad_new['W'+str(i)]\n",
        "                  theta['b' + str(i)] -= (lr/ (np.sqrt(v['b' + str(i)] + eps)) )*grad_new['b'+str(i)]\n",
        "\n",
        "              for i in range(1, num_hidden_layer + 2):\n",
        "                  prev_v['W' + str(i)] = v['W' + str(i)]\n",
        "                  prev_v['b' + str(i)] = v['b' + str(i)]\n",
        "\n",
        "\n",
        "        acc = calculate_accuracy(x_train,y_train,theta,activ_fun, num_hidden_layer,input_size)\n",
        "        a, _, y_pred = forword_prop(x_train.T,theta,activ_fun, num_hidden_layer,input_size)\n",
        "        loss = cross_entropy(y_train.T, y_pred)\n",
        "        losses.append(loss)\n",
        "        validation_accuracy = calculate_accuracy(X_valid,Y_valid,theta,activ_fun, num_hidden_layer,input_size)\n",
        "        print(f\"Epoch {epoch+1}, Train_Loss: {loss} , Train_Accuracy: {acc*100},Test accutacy: {validation_accuracy*100}\")\n",
        "\n",
        "    # Plot the loss function over epochs\n",
        "    plt.plot(losses)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss over Epochs')\n",
        "    plt.show()\n",
        "\n",
        "    return theta\n",
        "\n",
        "# Optimizer Adam\n",
        "def opt_Adam(lr, theta, x_train, y_train, X_valid, Y_valid, activ_fun, epochs, num_hidden_layer, input_size,batch_size,eps, beta1=0.9,beta2= 0.99):\n",
        "    losses = []\n",
        "    length = x_train.shape[0]\n",
        "    v = {key: np.zeros_like(value) for key, value in theta.items()}\n",
        "    prev_v = {key: np.zeros_like(value) for key, value in theta.items()}\n",
        "    v_cap = {key: np.zeros_like(value) for key, value in theta.items()}\n",
        "\n",
        "    m = {key: np.zeros_like(value) for key, value in theta.items()}\n",
        "    prev_m = {key: np.zeros_like(value) for key, value in theta.items()}\n",
        "    m_cap = {key: np.zeros_like(value) for key, value in theta.items()}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        t = 0\n",
        "        no_of_pts = 0\n",
        "        loss = 0\n",
        "        grad_new = {key: np.zeros_like(value) for key, value in theta.items()}\n",
        "        for j in range(length):\n",
        "            x = x_train[j, :].reshape(-1, 1)\n",
        "            y_actual = y_train[j, :].reshape(-1, 1)\n",
        "            grad = back_prop(x, y_actual,activ_fun, num_hidden_layer, theta, input_size)\n",
        "            for i in range(num_hidden_layer+1, 0, -1):\n",
        "                grad_new['W'+str(i)] += grad['W'+str(i)]\n",
        "                grad_new['b'+str(i)] += grad['b'+str(i)]\n",
        "\n",
        "            no_of_pts = no_of_pts + 1\n",
        "            if no_of_pts % batch_size == 0:\n",
        "              t = t + 1\n",
        "\n",
        "              for i in range(1, num_hidden_layer + 2):\n",
        "                  m['W' + str(i)] = beta1 * prev_m['W' + str(i)] + (1-beta1)* grad_new['W'+str(i)]\n",
        "                  m['b' + str(i)] = beta1 * prev_m['b' + str(i)] + (1-beta1)* grad_new['b'+str(i)]\n",
        "\n",
        "              for i in range(1, num_hidden_layer + 2):\n",
        "                  m_cap['W' + str(i)] = (1/(1-np.power(beta1,t))) * m['W' + str(i)]\n",
        "                  m_cap['b' + str(i)] = (1/(1-np.power(beta1,t))) * m['b' + str(i)]\n",
        "                  prev_m['W' + str(i)] = m['W' + str(i)]\n",
        "                  prev_m['b' + str(i)] = m['b' + str(i)]\n",
        "\n",
        "              for i in range(1, num_hidden_layer + 2):\n",
        "                  v['W' + str(i)] = beta2 * prev_v['W' + str(i)] + (1-beta2)* ((grad_new['W'+str(i)])**2)\n",
        "                  v['b' + str(i)] = beta2 * prev_v['b' + str(i)] + (1-beta2)* ((grad_new['b'+str(i)])**2)\n",
        "\n",
        "              for i in range(1, num_hidden_layer + 2):\n",
        "                  v_cap['W' + str(i)] = (1/(1-np.power(beta2,t))) * v['W' + str(i)]\n",
        "                  v_cap['b' + str(i)] = (1/(1-np.power(beta2,t))) * v['b' + str(i)]\n",
        "                  prev_v['W' + str(i)] = v['W' + str(i)]\n",
        "                  prev_v['b' + str(i)] = v['b' + str(i)]\n",
        "\n",
        "\n",
        "              for i in range(1, num_hidden_layer + 2):\n",
        "                  # Update parameters\n",
        "                  theta['W' + str(i)] -= (lr/ (np.sqrt(v_cap['W' + str(i)] + eps)) )*(m_cap['W'+str(i)])\n",
        "                  theta['b' + str(i)] -= (lr/ (np.sqrt(v_cap['b' + str(i)] + eps)) )*(m_cap['b'+str(i)])\n",
        "\n",
        "\n",
        "        acc = calculate_accuracy(x_train,y_train,theta,activ_fun, num_hidden_layer,input_size)\n",
        "        a, _, y_pred = forword_prop(x_train.T,theta,activ_fun, num_hidden_layer,input_size)\n",
        "        loss = cross_entropy(y_train.T, y_pred)\n",
        "        losses.append(loss)\n",
        "        validation_accuracy = calculate_accuracy(X_valid,Y_valid,theta,activ_fun, num_hidden_layer,input_size)\n",
        "        print(f\"Epoch {epoch+1}, Train_Loss: {loss} , Train_Accuracy: {acc*100},Test accutacy: {validation_accuracy*100}\")\n",
        "    # Plot the loss function over epochs\n",
        "    plt.plot(losses)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss over Epochs')\n",
        "    plt.show()\n",
        "\n",
        "    return theta\n",
        "\n",
        "# Optimizer Nadam\n",
        "def opt_Nadam(lr, theta, x_train, y_train,X_valid, Y_valid,activ_fun, epochs, num_hidden_layer, input_size, batch_size,eps, beta1=0.9,beta2=0.99):\n",
        "    losses = []\n",
        "    length = x_train.shape[0]\n",
        "    v = {key: np.zeros_like(value) for key, value in theta.items()}\n",
        "    prev_v = {key: np.zeros_like(value) for key, value in theta.items()}\n",
        "    v_cap = {key: np.zeros_like(value) for key, value in theta.items()}\n",
        "\n",
        "    m = {key: np.zeros_like(value) for key, value in theta.items()}\n",
        "    prev_m = {key: np.zeros_like(value) for key, value in theta.items()}\n",
        "    m_cap = {key: np.zeros_like(value) for key, value in theta.items()}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        t = 0\n",
        "        no_of_pts = 0\n",
        "        loss = 0\n",
        "        grad_new = {key: np.zeros_like(value) for key, value in theta.items()}\n",
        "        for j in range(length):\n",
        "            x = x_train[j, :].reshape(-1, 1)\n",
        "            y_actual = y_train[j, :].reshape(-1, 1)\n",
        "            grad = back_prop(x, y_actual,activ_fun, num_hidden_layer, theta, input_size)\n",
        "            for i in range(num_hidden_layer+1, 0, -1):\n",
        "                grad_new['W'+str(i)] += grad['W'+str(i)]\n",
        "                grad_new['b'+str(i)] += grad['b'+str(i)]\n",
        "\n",
        "            no_of_pts += 1\n",
        "            if no_of_pts % batch_size == 0:\n",
        "              t = t + 1\n",
        "\n",
        "              for i in range(1, num_hidden_layer + 2):\n",
        "                  m['W' + str(i)] = beta1 * prev_m['W' + str(i)] + (1-beta1)* grad_new['W'+str(i)]\n",
        "                  m['b' + str(i)] = beta1 * prev_m['b' + str(i)] + (1-beta1)* grad_new['b'+str(i)]\n",
        "\n",
        "              for i in range(1, num_hidden_layer + 2):\n",
        "                  m_cap['W' + str(i)] = (1/(1-np.power(beta1,t))) * m['W' + str(i)]\n",
        "                  m_cap['b' + str(i)] = (1/(1-np.power(beta1,t))) * m['b' + str(i)]\n",
        "                  prev_m['W' + str(i)] = m['W' + str(i)]\n",
        "                  prev_m['b' + str(i)] = m['b' + str(i)]\n",
        "\n",
        "              for i in range(1, num_hidden_layer + 2):\n",
        "                  v['W' + str(i)] = beta2 * prev_v['W' + str(i)] + (1-beta2)* ((grad_new['W'+str(i)])**2)\n",
        "                  v['b' + str(i)] = beta2 * prev_v['b' + str(i)] + (1-beta2)* ((grad_new['b'+str(i)])**2)\n",
        "\n",
        "              for i in range(1, num_hidden_layer + 2):\n",
        "                  v_cap['W' + str(i)] = (1/(1-np.power(beta2,t))) * v['W' + str(i)]\n",
        "                  v_cap['b' + str(i)] = (1/(1-np.power(beta2,t))) * v['b' + str(i)]\n",
        "                  prev_v['W' + str(i)] = v['W' + str(i)]\n",
        "                  prev_v['b' + str(i)] = v['b' + str(i)]\n",
        "\n",
        "\n",
        "              for i in range(1, num_hidden_layer + 2):\n",
        "                  # Update parameters\n",
        "                  theta['W' + str(i)] -= (lr/ (np.sqrt(v_cap['W' + str(i)] + eps)) )*(beta1*(m_cap['W'+str(i)])+((1-beta1)*(grad_new['W'+str(i)]))/(1-np.power(beta1,t)))\n",
        "                  theta['b' + str(i)] -= (lr/ (np.sqrt(v_cap['b' + str(i)] + eps)) )*(beta1*(m_cap['b'+str(i)])+((1-beta1)*(grad_new['b'+str(i)]))/(1-np.power(beta1,t)))\n",
        "\n",
        "        acc = calculate_accuracy(x_train,y_train,theta,activ_fun, num_hidden_layer,input_size)\n",
        "        a, _, y_pred = forword_prop(x_train.T,theta,activ_fun, num_hidden_layer,input_size)\n",
        "        loss = cross_entropy(y_train.T, y_pred)\n",
        "        losses.append(loss)\n",
        "        test_accuracy = calculate_accuracy(X_valid,Y_valid,theta,activ_fun, num_hidden_layer,input_size)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Train_Loss: {loss} , Train_Accuracy: {acc*100},Test accutacy: {test_accuracy*100}\")\n",
        "\n",
        "    plt.plot(losses)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss over Epochs')\n",
        "    plt.show()\n",
        "\n",
        "    return theta\n",
        "\n",
        "\"\"\"Best configuration hyperparameters\"\"\"\n",
        "\n",
        "ideal_epochs = 10\n",
        "ideal_weight = 'xavier'\n",
        "ideal_activ_func = 'relu'\n",
        "ideal_hidden_size = 128\n",
        "ideal_batch_size = 64\n",
        "ideal_optimizer = 'RMSprop'\n",
        "ideal_hidden_layer = 5\n",
        "ideal_weight_decay = 0\n",
        "ideal_learning_rate = 0.0001\n",
        "\n",
        "W_b1 = init_network( num_hidden_layer=5, num_nodes_hidden_layers=128, weight='xavier',input_size=784 , output_size=10 )\n",
        "W_b_rms= opt_RMSprop(0.0001, W_b1, X_train, Y_train, X_test, Y_test,activ_fun='relu', epochs=10, num_hidden_layer=5, input_size=784 ,batch_size=64, eps=0.01, beta=0.9)\n",
        "y_test_label = []\n",
        "y_test_pred_label = []\n",
        "l = X_test.shape[0]\n",
        "for i in range(l):\n",
        "  X_tes = X_test[i,:].reshape(-1, 1)\n",
        "  a,h,y_test_pred = forword_prop(X_tes,W_b_rms, activ_fun='relu', num_hidden_layer=5,input_size=784)\n",
        "  y1 = np.argmax(Y_test[i,:].reshape(-1, 1), axis = 0)\n",
        "  y_test_label.append(y1)\n",
        "  y2 = np.argmax(y_test_pred, axis = 0)\n",
        "  y_test_pred_label.append(y2)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "cm = confusion_matrix(y_test_label[:10000], y_test_pred_label[:10000])\n",
        "\n",
        "class_labels = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\",\"Coat\",\"Sandal\",\"Shirt\", \"Sneaker\", \"Bag\",\"Ankle boot\"]\n",
        "\n",
        "cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "cm_display.plot(ax=ax, cmap=plt.cm.Reds)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "\"\"\"Prediction on Mnist Data set for three different configurations\n",
        "\n",
        "Question-10\n",
        "\"\"\"\n",
        "\n",
        "from keras.datasets import mnist\n",
        "(Xtrain, ytrain), (Xtest, ytest) = mnist.load_data()\n",
        "print(Xtrain.shape, ytrain.shape, Xtest.shape, ytest.shape)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Flatten the image data\n",
        "X_train = Xtrain.reshape(Xtrain.shape[0], -1)\n",
        "X_test = Xtest.reshape(Xtest.shape[0], -1)\n",
        "\n",
        "# Normalize the image data\n",
        "MX_train = X_train / 255.0\n",
        "MX_test = X_test / 255.0\n",
        "\n",
        "def one_hot_encod(arr):\n",
        "  mat = np.zeros((len(arr),10))\n",
        "  for i in range(len(arr)):\n",
        "    mat[i,arr[i]] = 1\n",
        "  return mat\n",
        "# Convert labels to one-hot encoding\n",
        "MY_train =  one_hot_encod(ytrain)\n",
        "MY_test =  one_hot_encod(ytest)\n",
        "print(\"Training images shape:\", MX_train.shape)\n",
        "print(\"Training labels shape:\", MY_train.shape)\n",
        "print(\"Testing images shape:\", MX_test.shape)\n",
        "print(\"Testing labels shape:\", MY_test.shape)\n",
        "\n",
        "\"\"\"Best configuration with optimizers\n",
        "\n",
        "Configuration 1\n",
        "\"\"\"\n",
        "\n",
        "#---------------------------------------------------\n",
        "#lr = 0.0001 , num_hidden_layer = 5 , num_nodes_hidden_layers= 128, weight = xavior, activ_fun = 'relu' , epoch = 10 , batch_size = 64\n",
        "#----------------------------------------------------\n",
        "W_b1 = init_network( num_hidden_layer=5, num_nodes_hidden_layers=128, weight='xavier',input_size=784 , output_size=10 )\n",
        "Wb1 =opt_RMSprop(0.0001, W_b1, MX_train, MY_train,MX_test, MY_test,activ_fun='relu', epochs=10, num_hidden_layer=5, input_size=784 ,batch_size=64, eps=0.01, beta=0.9)\n",
        "\n",
        "\"\"\"Configuration 2\"\"\"\n",
        "\n",
        "#---------------------------------------------------\n",
        "# lr = 0.0001 , num_hidden_layer = 3 , num_nodes_hidden_layers= 128, weight = xavior, activ_fun = 'sigmoid' , epoch = 10 , batch_size = 32\n",
        "#----------------------------------------------------\n",
        "W_b2 = init_network( num_hidden_layer=3, num_nodes_hidden_layers=128, weight='xavier',input_size=784 , output_size=10 )\n",
        "Wb2 = opt_Adam(0.0001,  W_b2, MX_train, MY_train,MX_test, MY_test, activ_fun='sigmoid', epochs=10, num_hidden_layer=3, input_size=784,batch_size=32 ,eps=0.01, beta1=0.9,beta2= 0.99)\n",
        "\n",
        "\"\"\"Configuration 3\"\"\"\n",
        "\n",
        "#---------------------------------------------------\n",
        "# lr = 0.0001 , num_hidden_layer = 3 , num_nodes_hidden_layers= 64, weight = xavior, activ_fun = 'relu' , epoch = 10 , batch_size = 32\n",
        "#----------------------------------------------------\n",
        "W_b3 = init_network(num_hidden_layer=3, num_nodes_hidden_layers=64, weight='xavier',input_size=784 , output_size=10)\n",
        "Wb3 = opt_Nadam(0.0001, W_b3, MX_train, MY_train,MX_test, MY_test,activ_fun='relu', epochs=10, num_hidden_layer=3, input_size=784,batch_size=32,eps=0.01, beta1=0.9,beta2=0.99)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "hJtx_X5OVAk6",
        "outputId": "91b054d5-59dd-4e62-9438-d366f85a6d38"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-57842b5c6a61>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0mW_b1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_network\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnum_hidden_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nodes_hidden_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'xavier'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m784\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m \u001b[0mW_b_rms\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mopt_RMSprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_b1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactiv_fun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hidden_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m784\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0my_test_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0my_test_pred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zjt-aIqIVp3L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}