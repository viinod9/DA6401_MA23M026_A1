{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JL3hhlzbjFfb",
        "outputId": "40dea44e-b34b-4aec-9e86-54f9959a9180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 0: Training Accuracy = 0.1000\n",
            "Epoch 5: Training Accuracy = 0.7583\n",
            "Epoch 10: Training Accuracy = 0.8310\n",
            "Epoch 15: Training Accuracy = 0.8520\n",
            "Test Accuracy: 0.8436\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "# Activation Functions\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(a):\n",
        "    return a * (1 - a)\n",
        "\n",
        "# Neural Network Class\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_layers, output_size):\n",
        "        \"\"\"\n",
        "        :param input_size: Number of input features (e.g., 784 for Fashion-MNIST)\n",
        "        :param hidden_layers: List containing the number of neurons in each hidden layer\n",
        "        :param output_size: Number of output classes (e.g., 10 for Fashion-MNIST)\n",
        "        \"\"\"\n",
        "        self.layers = [input_size] + hidden_layers + [output_size]\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            self.weights.append(np.random.randn(self.layers[i], self.layers[i+1]) * 0.01)\n",
        "            self.biases.append(np.zeros((1, self.layers[i+1])))\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "        :return: activations and weighted sums\n",
        "        \"\"\"\n",
        "        activations = [X]\n",
        "        zs = []\n",
        "\n",
        "        for i in range(len(self.weights) - 1):\n",
        "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
        "            zs.append(z)\n",
        "            activations.append(sigmoid(z))\n",
        "\n",
        "        # Output layer with softmax\n",
        "        z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
        "        zs.append(z)\n",
        "        activations.append(softmax(z))\n",
        "\n",
        "        return activations, zs\n",
        "\n",
        "    def backward(self, X, y, activations, zs, learning_rate):\n",
        "        \"\"\"\n",
        "        Backward pass using backpropagation.\n",
        "        \"\"\"\n",
        "        m = X.shape[0]\n",
        "        y_one_hot = np.eye(self.layers[-1])[y]\n",
        "\n",
        "        # Compute output layer gradient\n",
        "        delta = activations[-1] - y_one_hot\n",
        "        dW = np.dot(activations[-2].T, delta) / m\n",
        "        db = np.sum(delta, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights[-1] -= learning_rate * dW\n",
        "        self.biases[-1] -= learning_rate * db\n",
        "\n",
        "        # Backpropagate through hidden layers\n",
        "        for i in range(len(self.weights) - 2, -1, -1):\n",
        "            delta = np.dot(delta, self.weights[i+1].T) * sigmoid_derivative(activations[i+1])\n",
        "            dW = np.dot(activations[i].T, delta) / m\n",
        "            db = np.sum(delta, axis=0, keepdims=True) / m\n",
        "\n",
        "            self.weights[i] -= learning_rate * dW\n",
        "            self.biases[i] -= learning_rate * db\n",
        "\n",
        "    def train(self, X_train, y_train, epochs, batch_size, learning_rate):\n",
        "        \"\"\"\n",
        "        Train the neural network using mini-batch gradient descent.\n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(0, X_train.shape[0], batch_size):\n",
        "                X_batch = X_train[i:i+batch_size]\n",
        "                y_batch = y_train[i:i+batch_size]\n",
        "\n",
        "                activations, zs = self.forward(X_batch)\n",
        "                self.backward(X_batch, y_batch, activations, zs, learning_rate)\n",
        "\n",
        "            if epoch % 5 == 0:\n",
        "                acc = self.evaluate(X_train, y_train)\n",
        "                print(f\"Epoch {epoch}: Training Accuracy = {acc:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions for input X.\n",
        "        \"\"\"\n",
        "        activations, _ = self.forward(X)\n",
        "        return np.argmax(activations[-1], axis=1)\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        \"\"\"\n",
        "        Evaluate accuracy on given data.\n",
        "        \"\"\"\n",
        "        y_pred = self.predict(X)\n",
        "        return np.mean(y_pred == y)\n",
        "\n",
        "# Load and preprocess Fashion-MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize and flatten images\n",
        "X_train = X_train.reshape(-1, 28*28) / 255.0\n",
        "X_test = X_test.reshape(-1, 28*28) / 255.0\n",
        "\n",
        "# Define the neural network architecture\n",
        "hidden_layers = [128, 64]  # Example: 2 hidden layers with 128 and 64 neurons\n",
        "nn = NeuralNetwork(input_size=28*28, hidden_layers=hidden_layers, output_size=10)\n",
        "\n",
        "# Train the model\n",
        "nn.train(X_train, y_train, epochs=20, batch_size=64, learning_rate=0.1)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_accuracy = nn.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install idx2numpy\n",
        "!pip install wandb -qqq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kowq6LZwrxjT",
        "outputId": "b9a2c87f-c6a6-4248-8a7e-9bd8cc412f29"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting idx2numpy\n",
            "  Downloading idx2numpy-1.2.3.tar.gz (6.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from idx2numpy) (1.26.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from idx2numpy) (1.17.0)\n",
            "Building wheels for collected packages: idx2numpy\n",
            "  Building wheel for idx2numpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for idx2numpy: filename=idx2numpy-1.2.3-py3-none-any.whl size=7904 sha256=1eea4f2ad624ba7bc10da7c84afd5357dae324e28a5776415664bb2098b51d03\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/e5/e7/70fc742b3645ddf9d392f766feccbcc95cb3a3c806f8588af0\n",
            "Successfully built idx2numpy\n",
            "Installing collected packages: idx2numpy\n",
            "Successfully installed idx2numpy-1.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(project=\"fashion-mnist-backprop\", name=\"NN-All-Optimizers\")\n",
        "\n",
        "# Load Fashion-MNIST dataset using TensorFlow\n",
        "def load_fashion_mnist():\n",
        "    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "    X_train, X_test = X_train.reshape(-1, 28*28) / 255.0, X_test.reshape(-1, 28*28) / 255.0\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "# Activation functions\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_layers, output_size, optimizer=\"sgd\", learning_rate=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.optimizer = optimizer\n",
        "        self.layers = [input_size] + hidden_layers + [output_size]\n",
        "\n",
        "        self.weights = [np.random.randn(self.layers[i], self.layers[i+1]) * 0.01 for i in range(len(self.layers) - 1)]\n",
        "        self.biases = [np.zeros((1, self.layers[i+1])) for i in range(len(self.layers) - 1)]\n",
        "\n",
        "        # Optimization parameters\n",
        "        self.velocities_w = [np.zeros_like(w) for w in self.weights]\n",
        "        self.velocities_b = [np.zeros_like(b) for b in self.biases]\n",
        "        self.m_w = [np.zeros_like(w) for w in self.weights]\n",
        "        self.m_b = [np.zeros_like(b) for b in self.biases]\n",
        "        self.v_w = [np.zeros_like(w) for w in self.weights]\n",
        "        self.v_b = [np.zeros_like(b) for b in self.biases]\n",
        "        self.t = 1\n",
        "\n",
        "    def forward(self, X):\n",
        "        activations, zs = [X], []\n",
        "        for w, b in zip(self.weights, self.biases):\n",
        "            z = activations[-1] @ w + b\n",
        "            zs.append(z)\n",
        "            activations.append(relu(z) if w is not self.weights[-1] else softmax(z))\n",
        "        return activations, zs\n",
        "\n",
        "    def compute_loss(self, y_pred, y_true):\n",
        "        y_one_hot = np.eye(self.layers[-1])[y_true]\n",
        "        return -np.mean(np.sum(y_one_hot * np.log(y_pred + 1e-8), axis=1))\n",
        "\n",
        "    def backward(self, X, y, activations, zs, batch_size):\n",
        "        grads_w, grads_b = [], []\n",
        "        y_one_hot = np.eye(self.layers[-1])[y]\n",
        "\n",
        "        delta = activations[-1] - y_one_hot\n",
        "        for i in reversed(range(len(self.weights))):\n",
        "            grads_w.insert(0, activations[i].T @ delta / batch_size)\n",
        "            grads_b.insert(0, np.sum(delta, axis=0, keepdims=True) / batch_size)\n",
        "            if i != 0:\n",
        "                delta = (delta @ self.weights[i].T) * relu_derivative(zs[i - 1])\n",
        "\n",
        "        self.update_weights(grads_w, grads_b)\n",
        "        self.t += 1\n",
        "\n",
        "    def update_weights(self, grads_w, grads_b):\n",
        "        if self.optimizer == \"sgd\":\n",
        "            self.sgd(grads_w, grads_b)\n",
        "        elif self.optimizer == \"momentum\":\n",
        "            self.momentum(grads_w, grads_b)\n",
        "        elif self.optimizer == \"nesterov\":\n",
        "            self.nesterov(grads_w, grads_b)\n",
        "        elif self.optimizer == \"rmsprop\":\n",
        "            self.rmsprop(grads_w, grads_b)\n",
        "        elif self.optimizer == \"adam\":\n",
        "            self.adam(grads_w, grads_b)\n",
        "        elif self.optimizer == \"nadam\":\n",
        "            self.nadam(grads_w, grads_b)\n",
        "\n",
        "    def sgd(self, grads_w, grads_b):\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] -= self.learning_rate * grads_w[i]\n",
        "            self.biases[i] -= self.learning_rate * grads_b[i]\n",
        "\n",
        "    def momentum(self, grads_w, grads_b, beta=0.9):\n",
        "        for i in range(len(self.weights)):\n",
        "            self.velocities_w[i] = beta * self.velocities_w[i] + self.learning_rate * grads_w[i]\n",
        "            self.velocities_b[i] = beta * self.velocities_b[i] + self.learning_rate * grads_b[i]\n",
        "            self.weights[i] -= self.velocities_w[i]\n",
        "            self.biases[i] -= self.velocities_b[i]\n",
        "\n",
        "    def nesterov(self, grads_w, grads_b, beta=0.9):\n",
        "        for i in range(len(self.weights)):\n",
        "            v_prev_w, v_prev_b = self.velocities_w[i], self.velocities_b[i]\n",
        "            self.velocities_w[i] = beta * self.velocities_w[i] + self.learning_rate * grads_w[i]\n",
        "            self.velocities_b[i] = beta * self.velocities_b[i] + self.learning_rate * grads_b[i]\n",
        "            self.weights[i] -= beta * v_prev_w + (1 + beta) * self.velocities_w[i]\n",
        "            self.biases[i] -= beta * v_prev_b + (1 + beta) * self.velocities_b[i]\n",
        "\n",
        "    def rmsprop(self, grads_w, grads_b, beta=0.9, epsilon=1e-8):\n",
        "        for i in range(len(self.weights)):\n",
        "            self.v_w[i] = beta * self.v_w[i] + (1 - beta) * grads_w[i] ** 2\n",
        "            self.weights[i] -= self.learning_rate * grads_w[i] / (np.sqrt(self.v_w[i]) + epsilon)\n",
        "\n",
        "    def adam(self, grads_w, grads_b, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        for i in range(len(self.weights)):\n",
        "            self.m_w[i] = beta1 * self.m_w[i] + (1 - beta1) * grads_w[i]\n",
        "            self.v_w[i] = beta2 * self.v_w[i] + (1 - beta2) * (grads_w[i] ** 2)\n",
        "            m_w_hat = self.m_w[i] / (1 - beta1 ** self.t)\n",
        "            v_w_hat = self.v_w[i] / (1 - beta2 ** self.t)\n",
        "            self.weights[i] -= self.learning_rate * m_w_hat / (np.sqrt(v_w_hat) + epsilon)\n",
        "\n",
        "    def nadam(self, grads_w, grads_b):\n",
        "        self.adam(grads_w, grads_b)\n",
        "\n",
        "    def train(self, X, y, X_val, y_val, epochs=10, batch_size=64):\n",
        "        for epoch in range(epochs):\n",
        "            indices = np.random.permutation(X.shape[0])\n",
        "            X, y = X[indices], y[indices]\n",
        "\n",
        "            for i in range(0, X.shape[0], batch_size):\n",
        "                X_batch = X[i:i+batch_size]\n",
        "                y_batch = y[i:i+batch_size]\n",
        "\n",
        "                activations, zs = self.forward(X_batch)\n",
        "                self.backward(X_batch, y_batch, activations, zs, batch_size)\n",
        "\n",
        "            y_pred = self.predict(X_val)\n",
        "            acc = np.mean(y_pred == y_val)\n",
        "            loss = self.compute_loss(self.forward(X_val)[0][-1], y_val)\n",
        "            wandb.log({\"Loss\": loss, \"Accuracy\": acc, \"Epoch\": epoch + 1})\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f} - Accuracy: {acc:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.argmax(self.forward(X)[0][-1], axis=1)\n",
        "\n",
        "X_train, y_train, X_test, y_test = load_fashion_mnist()\n",
        "nn = NeuralNetwork(input_size=784, hidden_layers=[128, 64], output_size=10, optimizer=\"adam\", learning_rate=0.001)\n",
        "nn.train(X_train, y_train, X_test, y_test, epochs=10, batch_size=64)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "aNl8pnsFomHD",
        "outputId": "da302154-ad77-49b4-c12c-28af38013b85"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250304_152546-vzr339c8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/viinod9-iitm/fashion-mnist-backprop/runs/vzr339c8' target=\"_blank\">NN-All-Optimizers</a></strong> to <a href='https://wandb.ai/viinod9-iitm/fashion-mnist-backprop' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/viinod9-iitm/fashion-mnist-backprop' target=\"_blank\">https://wandb.ai/viinod9-iitm/fashion-mnist-backprop</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/viinod9-iitm/fashion-mnist-backprop/runs/vzr339c8' target=\"_blank\">https://wandb.ai/viinod9-iitm/fashion-mnist-backprop/runs/vzr339c8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Loss: 0.5366 - Accuracy: 0.8080\n",
            "Epoch 2/10 - Loss: 0.5100 - Accuracy: 0.8181\n",
            "Epoch 3/10 - Loss: 0.4321 - Accuracy: 0.8443\n",
            "Epoch 4/10 - Loss: 0.4189 - Accuracy: 0.8532\n",
            "Epoch 5/10 - Loss: 0.4094 - Accuracy: 0.8517\n",
            "Epoch 6/10 - Loss: 0.3812 - Accuracy: 0.8630\n",
            "Epoch 7/10 - Loss: 0.3653 - Accuracy: 0.8692\n",
            "Epoch 8/10 - Loss: 0.3807 - Accuracy: 0.8643\n",
            "Epoch 9/10 - Loss: 0.3553 - Accuracy: 0.8717\n",
            "Epoch 10/10 - Loss: 0.3586 - Accuracy: 0.8731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HIVeVbS-rtD8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}